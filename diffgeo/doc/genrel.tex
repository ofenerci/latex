
\section{Differential geometry on general relativity}

\subsection{Introduction}
%
\theme{Source:} this section summarizes \cite{bertschinger:1999}.

\theme{Requirement:} general relativity (GR) requires familiarity with the geometrical properties of curved spacetime.

\theme{GR essential ideas:} a curved, four-dimensional pseudo-Riemannian manifold models spacetime. At every event (spacetime point), general relativity physics is locally special relativity physics. Mass (as mass and momentum flux) curves spacetime as described by Einstein's field equations.

\theme{Events:} Points in spacetime.

\theme{Covariant principle:} the laws of physics must be expressed in a form that is valid independently of any coordinate system used to label events.

\theme{Geometric description} of GR is better suited to develop physical understanding and thus used for theoretical work. Components and coordinate systems are useful for calculations \emph{once a coordinate system is chosen} .

\theme{Conventions and notation:} assume units where \lingo{speed of light of vacuum} is unity, $\klight = 1$. \emph{Greek} indices ranging from 0 to 3 will represent tensorial components. \lingo{Coordinate-free notation}, \lingo{abstract index notation}, and \lingo{Einstein's summation convention} are used. Vectors will be decorated with an arrow, $\vec v$, while covectors with a tilde, $\cov p$. Events will be denoted with calligraphic fonts, $\event P$. Event positions on spacetime by $\vec\pos\vat{\event P}$, or shortened to $\vec\pos$. Event coordinates $\tvec\pos\mu\vat{\event P}$, or shortened to $\tvec\pos\mu$. The flat spacetime metric (Minkowski metric) components are $\tvecfmet\mu\nu = \diag\tuple{-1,+1,+1,+1}$.


\subsection{Vectors and covectors}
%
\theme{GR math:} differential geometry: the math of smoothly curved (hyper-)surfaces: differentiable manifolds.

\theme{Geometry} models physics, thus keep in mind the geometrical interpretation of physical quantities.

\theme{Plan:} introduction of geometric objects in a coordinate-free manner. Then, coordinates to simplify calculations.

\theme{Main geometric objects:} scalars, vectors, covectors, and tensors.


\subsubsection{Vectors}
%
\theme{Vector:} a quantity with a magnitude and a direction. Vectors form a linear space (linear algebra, \aka\ vector space); \ie, it's possible to add vectors and multiply them by scalars. Addition is closed.

Scalars and vectors are \lingo{invariant} under coordinate transformations; vector \emph{components} are not.

\theme{Events:} denote events in spacetime by $\event P$. This notation refers to a point in spacetime \emph{not} to a vector. So there's no need to think of vectors going from an origin to a point.

\theme{Separation:} as long as space is smooth (as assumed by considering a differentiable manifold), it's possible to define the separation (vector) $d\vec\sep$ between two infinitesimally close events, say $\event P,\event Q$. The set of all $d\vec\sep_\event{P}$ defines a tangent space at $\event P$. 

\theme{Vector field:} by assigning a tangent vector to \emph{every} event, we define a vector field at $\event P$.


\subsubsection{Covectors and dual vector space}
%
\theme{Covector:} defined as a linear scalar function of a vector; \ie, a covector takes a vector as input and outputs a scalar. 

Consider a covector $\cov p$, a linear space $\lspace V$, and a vector $\vec v\in\lspace V$. Then, 
%
\begin{equation*}
  \fdef{\cov p}{\lspace V}{\rspace}\suchthat\fmap{\vec v}{\rspace}\,.
\end{equation*}

\theme{Scalar product:} the action of $\cov p$ on $\vec v$, $\cov p\vat{\vec v}$, denoted with angle brackets:
%
\begin{equation}\label{eq:scalarproductcovectoronvector}
  \cov p\vat{\vec v} \defas \sprod{\cov p}{\vec v}\,.
\end{equation}

\theme{Notation:} use a right arrow $\vslot$ to denote a vector argument of a function, a tilde $\cslot$ for a covector argument, and a check mark $\fslot$ for a taken argument.

\theme{Scalar product operator:} binary operator taking a covector and a vector to return a scalar, $s\in\rspace$
%
\begin{equation*}
  \fdef{\sprod\cslot\vslot}{\dlspace V\setprod\lspace V}{\rspace}\suchthat\fmap{\tuple{\cov p, \vec v}}{s}\,,
\end{equation*}
%
\theme{Note:} as defined, the scalar product works \emph{only} for covectors and vectors!

\theme{Covector as maps:} covectors are linear functions; \ie, consider two scalars $a,b\in\rspace$, two vectors $\vec v,\vec w$, and a covector $\cov p$. Then, $\cov p$ satisfies
%
\begin{align}\label{eq:covectorpropertiesasmap}
  \cov p\vat{a\vec v + b\vec w} &= \sprod{\cov p}{a\vec v + b\vec w}                  \,, && \eqtxt{action of $\cov p$ on a vector} \\\nonumber
                                &= a\sprod{\cov p}{\vec v} + b\sprod{\cov p}{\vec w}  \,, && \eqtxt{linearity}              \\\nonumber
                                &= a\cov p\vat{\vec v} + b\cov p\vat{\vec w}          \,. && \eqtxt{definition of scalar product} \nonumber
\end{align}

\theme{Vectors and covectors are different:} consider covectors as independent objects of any vector (covectors are just functions!).

\theme{Covector field:} association of a covector to every event.

\theme{Distinction between objects} Events $\event P$, covectors $\cov p$, and vectors $\vec v$ are all distinct, as seen by $\sprod{\cov p_\event{P}}{\vec v_\event{P}}$.

\theme{Dual vector space:} covectors obey their own (linear) algebra, distinct from that of vectors. Thus, given two scalars $a,b\in\rspace$ and two covectors $\cov p,\cov q$. Then, we may define the covector $a\cov p + b\cov q$ by its action on a vector $\vec v$ as
%
\begin{equation}\label{eq:dualvectorspace}
  \br{a\cov p + b\cov q}\vat{\vec v} = \sprod{a\cov p + b\cov q, \vec v} 
                                     = a\sprod{\cov p}{\vec v} + b\sprod{\cov q}{\vec v}
                                     = a\cov p\vat{\vec v} + b\cov q\vat{\vec v}\,.
\end{equation}

\theme{Note:} by \cref{eq:scalarproductcovectoronvector,eq:dualvectorspace}, vectors and covectors are linear operators on each other, producing scalars. Thus, we may write
%
\begin{equation*}
  \sprod{\cov p}{\vec v} = \cov p\vat{\vec v}
                         = \vec v\vat{\cov p}\,.
\end{equation*}
%
The set of all covectors is a linear space complementary to, but different from, the linear space of vectors.

\theme{Dual vector space:} call the set of all covectors \lingo{dual vector space}.

\theme{Necessary distinction} between vectors and covectors, since spacetime is curved.


\subsection{Tensors}
%
\theme{Tensor of order $\torder mn$:} defined as a scalar function of $m$ vectors and $n$ covectors, linear in all of its arguments. Refer to a tensor of order $\torder mn$ also as $\torder mn$-tensor.

\theme{Tensor types:} from the definition, scalars are $\torder 00$ tensors, vectors $\torder 10$ tensors, and covectors $\torder 10$ tensors.

\theme{Notation:} the $\torder mn$ notation does not distinguish were vectors and covectors are to be placed. To solve this, represent vectors by arrows and covectors by tildes in tensor slots; \eg, a $\torder 21$ tensor $t$ taking a vector in its first and third slots and a covector in the second will be noted as a linear function with two vector and one covector arguments: $t\vat{\vslot,\cslot,\vslot}$.

\theme{Identity tensor:} the scalar product is a $\torder 11$ tensor, denoted by $\idtens$ and called \lingo{identity tensor}
%
\begin{equation}\label{eq:identitytensordefinition}
  \idtens\vat{\cov p, \vec v} \defas \sprod{\cov p}{\vec v}
                              = \cov p\vat{\vec v}
                              = \vec v\vat{\cov p}\,.
\end{equation}
%
The identity tensor is a function that take a covector and a vector and returns a scalar, $s$:
%
\begin{equation*}
  \fdef{\idtens}{\dlspace V\setprod\lspace V}{\rspace}\suchthat
  \fmap{\tuple{\cov p,\vec v}}{\sprod{\cov p}{\vec v} = s}\,.
\end{equation*}
%
If a covector $\cov p$ is plugged into $\idtens$ but no vector, then $\idtens\vat{\cov p, \vslot} = \cov p$. Thus, a covector can be seen as a function waiting for a vector to produce a scalar. By a similar argument, a vector can be seen as a function waiting for a covector to return a scalar.

\theme{Notation:} reserve the angle bracket scalar product for the identity tensor.

\theme{Linearity:} a $\torder mn$ tensor is linear in all of its arguments. For instance, for $m = 2$ and $n = 0$, by extension of \cref{eq:covectorpropertiesasmap}, we have
%
\begin{equation}\label{eq:linearityoftensors}
  t\vat{a\cov p + b\cov q, c\cov r + d\cov s} = ac\,t\vat{\cov p, \cov r} 
                                              + ad\,t\vat{\cov p, \cov s} 
                                              + bc\,t\vat{\cov q, \cov r} 
                                              + bd\,t\vat{\cov q, \cov s}
\end{equation}
%
By extension of \cref{eq:dualvectorspace}, tensors of a given order form a linear algebra: a linear combination of $\torder mn$ tensors is also a $\torder mn$ tensor.

\theme{Equality:} two tensors of the same order are equal if they return the same scalar when applied to all possible input vectors and covectors.

\theme{Addition:} only tensors of the same order may be added or compared.

\theme{Note:} due to equality and addition, it's \emph{crucial} to keep track the order of each tensor.

\theme{Tensor fields:} association of a tensor to every event; just as scalar fields, vector fields, and covector fields.

\theme{Changing tensor order:} by i) tensor product, ii) contraction, and iii) taking its gradient.

\theme{Tensor product:} noted by $\tprod$, the tensor product combines two tensors of order $\torder mn$ and $\torder{m'}{n'}$ to form a tensor of order $\torder{m + m'}{n + n'}$ by simply combining the argument list of the two tensors and thereby expanding the dim of the tensor space. For instance, the tensor product of two vectors $\vec a,\vec b$ gives a $\torder 20$ tensor:
%
\begin{equation}\label{eq:vectorvectortensorproduct}
  t = \vec a\tprod\vec b\,,\quad t\vat{\cov p, \cov q} \defas \vec a\vat{\cov p}\vec b\vat{\cov q}\,,
\end{equation}
%
or, as a function, $\tprod:\fmap{\tuple{\vec a,\vec b}}{\vec a\vat{\cov p}\vec b\vat{\cov q}}$. 

\theme{Process:} remember that every vector can be seen as having a potential covector and every covector as having a potential vector. Thus, when forming a tensor $t = \vec a\tprod\vec b$, we are leaving the potential covectors in the arguments of $t$; \ie, $t\vat{\cov p,\cov q}$. So that, when $t$ is evaluated, it can return the scalar $\vec a\vat{\cov p}\vec b\vat{\cov q}$.

\theme{Example:} Say there is a $\torder 21$ tensor $t\vat{\cslot,\vslot,\vslot}$. Then, it can be seen as a product of a covector $\cov u$, a vector $\vec v$, and a vector $\vec w$; \ie, $t = \cov u\tprod\vec v\tprod\vec w$. Thus, when forming $t$ by the tensor product, we are leaving the dual of the forming entities in the argument list of $t$:
%
\begin{equation*}
  t\vat{\cslot,\vslot,\vslot}         \,,\quad 
  t = \cov u\tprod\vec v\tprod\vec w  \,,\quad
  t\vat{\vec a,\cov b,\cov c} = \cov u\vat{\vec a}\vec v\vat{\cov b}\vec w\vat{\cov c}\,.
\end{equation*}

\theme{Note:} the preceding reasoning will help, I hope, to understand tensors in coordinate bases.

\theme{Commutativity:} the tensor product is anti-commutative: $\vec a\tprod\vec b \neq \vec b\tprod\vec a$, since $\vec a\vat{\cov p}\vec b\vat{\cov q}\neq\vec a\vat{\cov q}\vec b\vat{\cov p}$, for all $\cov p$ and $\cov q$.

\theme{Contraction:} contraction reduces the order of a $\torder mr$ tensor to $\torder{m - 1}{n - 1}$.

\theme{Gradient:} to be discussed later.


\subsubsection{Metric tensor}
%
\theme{Extending scalar product:} as defined \cref{eq:scalarproductcovectoronvector}, the scalar product requires a covector and a vector. It can be extended to accept two vectors or two covectors by the definition of a tensor. Any $\torder 20$ tensor will give a scalar from two vectors and any $\torder 02$ tensor will give a scalar from two covectors. However, there is a special $\torder 20$ tensor field called the \lingo{metric} $\metric$ and a related $\torder 02$ tensor called the \lingo{inverse metric tensor} that deserve special attention.

\theme{Metric tensor:} a symmetric, bilinear scalar function of two vectors, $\metric\vat{\vslot,\vslot}$, or $\fdef{\metric}{\lspace V\setprod\lspace V}{\rspace}$. Consider two vectors $\vec v,\vec w$. Then $\metric$ returns a scalar, called the \lingo{dot product}:
%
\begin{equation}\label{eq:metricdotproduct}
  \metric\vat{\vec v, \vec w} = \vec v\iprod\vec w 
                              = \vec w\iprod\vec v 
                              = \metric\vat{\vec w,\vec v}\,.
\end{equation}

\theme{Inverse metric tensor:} a symmetric, bilinear scalar function of two covectors, $\metric\vat{\cslot,\cslot}$, or $\fdef{\invmet}{\dlspace V\setprod\dlspace V}{\rspace}$. Consider two covectors $\cov p,\cov q$. Then $\invmet$ returns a scalar, called the \lingo{dot product}:
%
\begin{equation}\label{eq:inversemetricdotproduct}
  \invmet\vat{\cov p, \cov q} = \cov p\iprod\cov q
                              = \cov q\iprod\cov p 
                              = \invmet\vat{\cov q,\cov p}\,.
\end{equation}

\theme{Note:} reserve the dot product notation for the metric and inverse metric tensors.

\theme{Property:} the metric allows us to convert vectors to covectors. If we forget to include a vector $\vec w$ in \cref{eq:metricdotproduct}, then we get a quantity, denoted $\cov v$, that behaves as a covector:
%
\begin{equation}\label{eq:metricasmap}
  \cov v\vat{\vslot} \defas \metric\vat{\vec v, \vslot}
                      = \metric\vat{\vslot, \vec v}\,,
\end{equation}
%
where we have inserted a $\vslot$ to remind ourselves that a vector must be inserted to return a scalar.

\theme{Metric as map:} $\metric$ acts as a map from the space of vectors to that of covectors: $\fdef{\metric}{\lspace V}{\dlspace V}$. By definition, $\invmet$ is the inverse map: it takes covectors and returns vectors: $\fdef{\invmet}{\dlspace V}{\lspace V}$.

\theme{Inverse metric as map:} if $\cov v$ is defined for any $\vec v$ by \cref{eq:metricasmap}, then the inverse metric $\invmet$ is defined by
%
\begin{equation}\label{eq:inversemetricasmap}
  \vec v\vat{\cslot} \defas \invmet\vat{\cov v, \cslot}
                      = \invmet\vat{\cslot, \cov v}\,.
\end{equation}

\theme{Scalars from vectors and covectors:} \cref{eq:identitytensordefinition} and \crefrange{eq:metricdotproduct}{eq:inversemetricasmap} give us several ways to obtain scalars from vectors $\vec v,\vec w$ and their associated covectors $\cov v,\cov w$:
%
\begin{equation}\label{eq:scalarsfromvecsandassociatedcovecs}
  \sprod{\cov v}{\vec w} = \sprod{\cov w}{\vec v}
                         = \vec v\iprod\vec w
                         = \cov v\iprod\cov w
                         = \idtens\vat{\cov v,\vec w}
                         = \idtens\vat{\cov w,\vec v}
                         = \metric\vat{\vec v,\vec w}
                         = \invmet\vat{\cov v,\cov w}\,.
\end{equation}

\theme{Pattern:} both the angle bracket product and the identity tensor accept a covector and a vector (in that order!), dot product accepts either two covectors or two vectors, metric tensor accepts only vectors, and inverse metric tensor accepts only covectors.

\theme{Summary:} see the efficiency of the methods. We defined only one product, the angle bracket product, to multiply covectors and vectors. Then, we use the metric to transform either the covector into a vector, or the vector into a covector, to get the scalar product of two vectors or of two covectors. That's why we needed the metric, to act as a transforming agent!


\subsubsection{Basis vectors and covectors}
%
\theme{Geometric formulation} of GR is best suited for understanding and theoretical work.

\theme{Coordinates} are better suited to perform calculations.

\theme{How to introduce coordinates?} Introduce a set of linearly independent basis vectors and a set of linearly independent basis covectors spanning our vector and dual vector spaces.

\theme{How many basis vectors?} The number of linearly independent basis vectors equals the dimensionality of the vector space. In our case, we need four basis vectors and four basis covectors, since the dimensionality of spacetime is four.

\theme{Basis vectors:} set of basis vector fields: $\bset{\tbvec\mu}{\mu = 0}{3}$, where the index $\mu$ labels the basis vector.

\theme{Note:} \emph{any} four linearly independent basis vectors at each event will work. We do \emph{not} impose orthogonality, nor orthonormality, \emph{nor} any other condition in general. There is \emph{no} implied relation to coordinates!

\theme{Vector projection:} given a basis, we may project (expand) \emph{any} vector field $\vec a$ as a linear combination of basis vectors:
%
\begin{equation}\label{eq:vectorprojectiononbasisvectors}
  \vec a = \tvec a\mu\tbvec\mu
         = \tvec a0\tbvec 0 + \tvec a1\tbvec 1 + \tvec a2\tbvec 2 + \tvec a3\tbvec 3\,.
\end{equation}
%
Note the pairing of superscripts with subscripts to satisfy Einstein's summation convention.

\theme{Contravariant components:} the coefficients $\tvec a\mu$ are called the components of the vector on the given basis, \aka\ contravariant components.

\theme{Important!} the components $\tvec a\mu$ depend on the basis vectors $\tbvec\mu$, but the vector $\vec a$ does \emph{not}!

\theme{Covector projection:} similarly, we may choose a basis of covectors on which to project (expand) covectors. Although \emph{any} four linearly independent covectors will suffice for each event, we \emph{prefer} to choose a special covector basis called the dual basis, denoted $\bset{\tbcov\mu}{\mu = 0}{3}$.

\theme{Relation basis vectors and basis vectors} So far, there is \emph{no} relation between basis vectors and basis covectors, not even given by the metric tensor. Rather, the dual basis (covector basis) is \emph{defined} by imposing the following requirements at each event:
%
\begin{equation}\label{eq:relationbasisvecbasiscovec}
  \sprod{\tbcov\mu}{\tbvec\mu} = \tkron\mu\nu\,,
\end{equation}
%
where $\kron$ is Kronecker's delta.

\theme{Note:} Kronecker's delta has \emph{always} a superscript and a subscript.

\theme{Covector projection:} Now, with the covector basis defined, we can expand any covector field $\cov p$ in the covector basis:
%
\begin{equation}\label{eq:vectorprojectiononbasiscovecs}
  \cov p = \tcov p\mu\tbcov\mu\,.
\end{equation}

\theme{Covariant components:} the coefficients $\tcov p\mu$ are called the covariant components.

\theme{Getting components:} there's a \scare{simple} way to get the components of vectors and covectors: use the fact that vectors are scalar functions of covectors and \vicvers. Thus, one simply evaluates the vector using the appropriate covector basis (action of $\vec a$ on $\tbcov\mu$):
%
\begin{align}\label{eq:veccomponentsusingcovecbasis}
  \vec a\vat{\tbcov\mu} &= \sprod{\tbcov\mu}{\vec a}               \,,&&\eqtxt{definition of covector}\\\nonumber
                        &= \sprod{\tbcov\mu}{\tvec a\nu\tbvec\nu}  \,,&&\eqtxt{expansion of $\vec a$ on $\tbvec\nu$}\\\nonumber
                        &= \sprod{\tbcov\mu}{\tbvec\nu}\tvec a\nu  \,,&&\eqtxt{linearity}\\\nonumber
                        &= \tkron\mu\nu\tvec a\nu                  \,,&&\eqtxt{definition of $\kron$}\\\nonumber
                        &= \tvec a\mu                              \,,&&\eqtxt{index gymnastics}\\\nonumber
\end{align}
%
similarly for a covectors:
%
\begin{equation}\label{eq:covcomponentsusingvecbasis}
  \cov a\vat{\tbvec\mu} = \sprod{\tbvec\mu}{\cov a}
                        = \sprod{\tbvec\mu}{\tcov a\nu\tbcov\nu}
                        = \sprod{\tbvec\mu}{\tbcov\nu}\tcov a\nu
                        = \tkron\mu\nu\tcov a\nu
                        = \tcov a\nu\,,
\end{equation}

\theme{Note} that to get a \emph{vector component} (a scalar), we need a function that takes a vector and returns a scalar: the angle bracket product. However, for the angle bracket product to work, we need to plug the vector whose component we need and a covector: the basis covector! Now all works and makes sense! By the same token, to get a \emph{covector component} (a scalar), we need the angle bracket again and the \emph{vector basis}.

\theme{Summary:} to get vector (contravariant) components, use the covector basis. To get covector (covariant) components, use the vector basis.


\subsubsection{Tensor algebra}
%
\theme{Tensor components:} use the same ideas to expand tensors as products of components and basis tensors. First, we note that a basis for a $\torder mn$ tensor is provided by the tensor product of $m$ vectors and $n$ covectors. For instance, consider the $\torder 20$ metric tensor. The metric tensor can be decomposed into basis tensors $\tbcov\mu\tprod\tbcov\nu$. The \lingo{components} of a $\torder mn$ tensor, labeled with $m$ superscripts and $n$ subscripts, are obtained by evaluating the tensor using $m$ basis covectors and $n$ basis vectors.

\theme{Tensor decomposition:} to decompose a vector, we need to use a basis covector. To decompose a covector, we need a basis vector. Thus, to decompose the metric tensor (that requires two vectors as input), we need two basis covectors forming a basis: $\tbcov\mu\tprod\tbcov\nu$.

\theme{Metric tensor and inverse metric tensor components:} the components of the $\torder 20$ metric tensor, the $\torder 02$ inverse metric tensor, and the $\torder 11$ identity tensor are
%
\begin{align}\label{eq:metricinversemetricidentitytensorscomponents}
  \tmetric\mu\nu &= \metric\vat{\tbvec\mu, \tbvec\nu} = \tbvec\mu\iprod\tbvec\nu \,,\\\nonumber
  \tinvmet\mu\nu &= \invmet\vat{\tbcov\mu, \tbcov\nu} = \tbcov\mu\iprod\tbcov\nu \,,\\\nonumber
  \tidtens\mu\nu &= \tkron\mu\nu = \idtens\vat{\tbcov\mu, \tbvec\nu} = \sprod{\tbcov\mu}{\tbvec\nu} \,.\\\nonumber
\end{align}

\theme{Proof:} $\metric$ eats up two vectors and returns a scalar: $\metric\vat{\vslot,\vslot}$, so it behaves like a covector. Then, $\metric$ can be thought of being the tensor product of two covectors: $\metric = \cov a\tprod\cov b$. On the other hand, to find the components of a covector $\cov a$, we used the basis vectors $\tbvec\mu$ and applied $\cov a$ to $\tbvec\mu$. Then, using the same technique as for covectors, we can apply the basis vectors to the metric tensor, we have:
%
\begin{align*}
  \metric\vat{\tbvec\mu,\tbvec\nu} &= \br{\cov a\tprod\cov b}\vat{\tbvec\mu,\tbvec\nu}  \,,\\
                                   &= \cov a\vat{\tbvec\mu}\cov b\vat{\tbvec\nu}        \,, \\
                                   &= \sprod{\cov a}{\tbvec\mu}\sprod{\cov b}{\tbvec\nu} \,, \\
                                   &= \sprod{\tcov a\alpha\tbcov\alpha}{\tbvec\mu}\sprod{\tcov b\beta\tbcov\beta}{\tbvec\nu} \,, \\
                                   &= \tcov a\alpha\tcov b\beta\sprod{\tbcov\alpha}{\tbvec\mu}\sprod{\tbcov\beta}{\tbvec\nu} \,, \\
                                   &= \tcov a\alpha\tcov b\beta\,\tkron\alpha\mu\tkron\beta\nu \,, \\
                                   &= \tcov a\mu\tcov b\nu  \,,\\
                                   &= \tmetric\mu\nu        \,.
\end{align*}

\theme{Note:} I don't like the last proof. It seems that something is wrong. Specially, $\metric = \vec a\tprod\vec b$.

\theme{Remember:} metric tensor: $\metric\vat{\vslot,\vslot}$, inverse metric tensor: $\invmet\vat{\cslot,\cslot}$, and identity tensor: $\idtens\vat{\cslot,\vslot}$.

\theme{Pattern:} metric tensor takes two vectors and returns a scalar, thus it eats up two basis vectors and uses the dot product. The inverse metric tensor takes two covectors and returns a scalar, thus it eats up two basis covectors and uses the dot product. The identity tensor takes a covector and a vector (in that order) and returns a scalar, thus it eats up a basis covector and a basis vector and uses the angle bracket product.

\theme{Process:} metric tensor takes two vectors and returns a scalar, thus, to find its components, we need two basis vectors and a product capable of acting on them to return a scalar. We need the dot product. Similarly, for the inverse metric tensor and the identity tensor.

\theme{Tensor expansion:} remember that vectors can be expanded into a vector basis, as $\tvec v\mu\tbvec\mu$, we want to do the same with higher-order tensors. Tensors are given by summing over the tensor product of basis vectors and covectors:
%
\begin{align}\label{eq:metricinvmetricidtensorexpansiononabasis}
  \metric &= \tmetric\mu\nu\,\tbcov\mu\tprod\tbcov\nu \,,\\\nonumber
  \invmet &= \tinvmet\mu\nu\,\tbvec\mu\tprod\tbvec\nu \,,\\\nonumber
  \idtens &= \tkron\mu\nu  \,\tbvec\mu\tprod\tbvec\nu   \,.\nonumber
\end{align}

\theme{Important!} Vector components are scalars. To expand a vector, we need a vector basis. But, to find its components into the basis, we need the covector basis and the angle bracket product to generate a scalar.

\theme{Crucial!} I cannot find a proof for \cref{eq:metricinvmetricidtensorexpansiononabasis}. I have only an argument: when having a covector (and the metric behaves like that), it is possible to expand it onto a basis of covectors: $\tcov p\mu\tbcov\mu$. Then, the $\metric$ needs a basis of covectors, say $\tbcov\mu\tprod\tbcov\nu$. Therefore, $\metric$ can be expanded on that basis: $\metric = \tmetric\mu\nu\,\tbcov\mu\tprod\tbcov\nu$.

\theme{Mixed products:} basis vectors and basis covectors allow us to represent any tensor equation using components. For instance, the dot product between two vectors and two covectors and the scalar product between a covector and a vector may be written using components as
%
\begin{equation}\label{eq:dotproductscalarproductincomponents}
  \vec a\iprod\vec b = \tmetric\mu\nu\,\tvec a\mu\tvec b\nu\,,\quad
  \sprod{\cov p}{\vec a} = \tcov p\mu\tvec a\mu\,,\quad
  \cov p\iprod\cov q = \tinvmet\mu\nu\tcov p\mu\tcov q\nu\,.
\end{equation}

\theme{Proof:} all follow by expanding vectors and covectors onto their basis, by applying the definitions of the metric tensor and the identity tensor, and by using index gymnastics, like the first equation:
%
\begin{equation*}
  \vec a\iprod\vec b = \tvec a\mu\tbvec\mu\iprod\tvec b\nu\tbvec\nu
                     = \tbvec\mu\iprod\tbvec\nu\,\tvec a\mu\tvec b\nu
                     = \tmetric\mu\nu\,\tvec a\mu\tvec b\nu\,.
\end{equation*}

\theme{Equality:} two tensors are equal if they are of the same order and if all of their components are equal. If two tensors are equal in one basis, then they are equal in any base.

\theme{Getting vectors out of covectors:} the metric tensor and the inverse metric tensor allow us to transform vectors into covectors and \vicvers.
%
\begin{equation}\label{eq:metricandinversemetrictransformvecsintocovecs}
  \tcov v\mu = \metric\vat{\tbvec\mu, \vec v} = \tmetric\mu\nu\tvec v\nu\,,\quad
  \tvec v\mu = \invmet\vat{\tbcov\mu, \cov v} = \tinvmet\mu\nu\tcov v\nu\,.
\end{equation}

\theme{Remember} that the metric tensor is a function that takes two vectors, thus every slot behaves as a covector; so, if we only input one vector, then $\metric$ acts as a covector: it waits for another vector to be input. This is how $\metric$ transforms vectors into covectors. The inverse metric tensor does the same, but for covectors into vectors:

Because \cref{eq:metricandinversemetrictransformvecsintocovecs} must hold for any vector $\vec v$, the matrix defined by $\tinvmet\mu\nu$ is the inverse of the matrix defined by $\tmetric\mu\nu$:
%
\begin{equation}\label{eq:metricandinversemetricrelationship}
  \tinvmet\mu\nu\tmetric\mu\nu = \tkron\mu\nu\,.
\end{equation}

\theme{Index gymnastics:} use the metric tensor and the inverse metric tensor to lower and raise indices on components:
%
\begin{equation}\label{eq:loweringrisingindices}
  \vec v\iprod\vec w = \tmetric\mu\nu\,\tvec v\mu\tvec w\nu
                     = \tvec v\mu\tcov w\mu
                     = \tcov v\nu\tvec w\nu
                     = \tinvmet\mu\nu\,\tcov v\mu\tcov w\nu\,.
\end{equation}

\theme{Note:} two of the indices must be pairing: one up, one down!

\theme{Metric tensor as transform:} use the metric tensor and the inverse metric to transform $\torder mn$ tensors into $\torder{m + k}{n - k}$, where $k = -m, -m+1, \dotsc, n$. For instance, consider a $\torder 21$ tensor 
%
\begin{equation}\label{eq:anexampleofatensor}
  \tensor{t}{^\mu_\nu_\lambda} = t\vat{\cslot,\vslot,\vslot}\,.
\end{equation}
%
\theme{Note}: by using the recommendations of \cref{sec:varioustensorrepresentations}, $t$ can be rewritten as
%
\begin{align*}
  \tensor{t}{^\mu_\nu_\lambda} &\implies t\vat{\tbcov\mu,\tbvec\nu,\tbvec\lambda} \,,\\
                               &\implies \tensor{t}{^\mu_\nu_\lambda}\,\tbvec\mu\tprod\tbcov\nu\tprod\tbcov\lambda\,,\\
                               &\implies t\vat{\cslot,\vslot,\vslot}\,.
\end{align*}

In \cref{eq:anexampleofatensor}, if we fail to plug in the covector $\tbcov\mu$, the result is the \emph{vector} $\tensor{t}{^\kappa_\nu_\lambda}\tbvec\kappa$ (in slot notation: $t\vat{\cslot,\fslot,\fslot}$ returns a \emph{vector}, since it behaves as an entity that \emph{needs} a covector to return a scalar!) This vector may be inserted into the metric tensor to give the components of a $\torder 30$ tensor:
%
\begin{equation}\label{eq:exampleofmetrictransformationofatensor}
  \tensor{t}{_\mu_\nu_\lambda} \defas \metric\vat{\tbvec\mu, \tensor{t}{^\kappa_\nu_\lambda}\tbvec\kappa}
                               = \tmetric\mu\kappa\tensor{t}{^\kappa_\nu_\lambda}\,.
\end{equation}
%
The tensor $t$ has now order $\torder 30$; since $\tensor{t}{_\mu_\nu_\lambda} = t\vat{\tbvec\mu,\tbvec\nu,\tbvec\lambda} = t\vat{\vslot,\vslot,\vslot}$.

We could now use the inverse metric tensor to raise the third index, say, giving us the components of a $\torder 21$ tensor:
%
\begin{equation}\label{eq:exampleofinversemetrictransformationofatensor}
  \tensor{t}{_\mu_\nu^\lambda} \defas \invmet\vat{\tbcov\lambda,\tensor{t}{_\mu_\nu_\kappa}\tbcov\kappa}
                               = \tinvmet\lambda\kappa\tensor{t}{_\mu_\nu_\kappa}
                               = \tinvmet\lambda\kappa\tmetric\mu\rho\tensor{t}{^\rho_\nu_\kappa}\,.
\end{equation}
%
(\scare{To raise the third index} means to transform the last slot from vector to covector. Thus, $\tensor{t}{_\mu_\nu^\lambda} = t\vat{\vslot,\vslot,\cslot}$, a $\torder 21$ tensor.)
%
In fact, there are $2^{m + n}$ different tensor spaces with ranks summing to $m + n$. The metric tensor or the inverse metric tensor allow all of these tensors to be transformed into each other.

\theme{Why distinction between vectors and covectors?} \Cref{eq:loweringrisingindices} tells us why to distinguish between vectors and covectors. The scalar product of two vectors requires the metric tensor, while that of two covectors requires the inverse metric tensor. Only in \emph{flat spacetime} in \emph{rectangular} coordinates with \emph{orthonormal} basis are they equal. In more general settings (curved spaces, or non-rectangular coordinates, or non-orthonormal basis), we have that $\tmetric\mu\nu\neq\tinvmet\mu\nu$. As a result, \emph{it is impossible} to define a coordinate system where $\tmetric\mu\nu = \tinvmet\mu\nu$ everywhere.


\subsubsection{Tensor contraction}
%
\theme{Changing a tensor order:} as mentioned, contraction changes the dimensionality of a tensor by lowering its order. 

\theme{Contraction} pairs two arguments of a $\torder mn$ tensor: one vector and one covector. The arguments are replaced by basis vectors and basis covectors and summed over. For instance, consider a $\torder 31$ tensor $t\vat{\cslot,\vslot,\vslot,\vslot}$. Then, contract its second vector argument (by pairing the first argument -- a covector -- with the third argument -- the second vector argument) to have
%
\begin{equation}\label{eq:examplefofthecontractionofatensor}
  t\vat{\vslot,\vslot} = \cont_{1,3}t\vat{\cslot,\vslot,\vslot,\vslot}
                       = \tkron\beta\alpha t\vat{\tbcov\alpha, \vslot, \tbvec\beta,\vslot}
                       = \sum_{\lambda = 0}^{3}t\vat{\tbcov\lambda, \vslot, \tbvec\lambda,\vslot}\,,
\end{equation}
%
resulting in a $\torder 20$ tensor with the same name $t$ as the non-contracted tensor, but with shorter argument list, and with components $t\vat{\vslot,\vslot} = t\vat{\tbvec\mu,\tbvec\nu} = \tensor{t}{_\mu_\nu}$. In index notation, the last equation can be written as
%
\begin{equation}\label{eq:examplefofthecontractionofatensorindexnotation}
  \tensor{t}{_\mu_\nu} = \tensor{t}{^\lambda_\mu_\lambda_\nu}\quad\eqtxt{summation over $\lambda$ implied} \,,
\end{equation}
%
where the indices of the contracted $t$ where calculated as $t\vat{\tbcov\lambda,\vslot,\tbvec\lambda,\vslot} = t\vat{\tbcov\lambda,\tbvec\mu,\tbvec\lambda,\tbvec\nu} = \tensor{t}{^\lambda_\mu_\lambda_\nu}$.

\theme{Note} that contraction may be performed on any pair of covariant and contravariant indices.


\subsubsection{Change of basis}
%
\theme{Change of basis:} since we made no restrictions when choosing the basis, we can choose an alternative basis to express vectors.

\theme{Basis transformation:} given a basis $\bset{\tbvec\mu}{\mu = 0}{3}$, define another basis $\bset{\tbvec{\aidx\mu}}{\aidx\mu = 0}{3}$, distinguished by placing a prime on the indices, as follows:
%
\begin{equation}\label{eq:changeofbasisvectors}
  \tbvec{\aidx\mu} = \ttransmat{\mu}{\aidx\mu}\tbvec\mu\,.
\end{equation}
%
\theme{Process:} begin with $\tbvec\mu$, the basis we want to transform. Then, transform it using the transformation matrix $\ttransmat{\mu}{\aidx\mu}$ to $\tbvec\mu\ttransmat{\mu}{\aidx\mu}$. See how the $\mu$ index is paired in $\ttransmat{\mu}{\aidx\mu}$ (up: old index $\mu$, down: new index $\aidx\mu$). Finally, form the transformed element using the unpaired index in the other side of the equation, $= \tbvec{\aidx\mu}$; that is,
%
\begin{equation*}
  \tbvec\mu\ttransmat{\mu}{\aidx\mu} \mapsto \tbvec{\aidx\mu}\,.
\end{equation*}

\theme{Reflexion:} the transformation matrix therefore acts as a function that takes basis vectors and returns basis vectors:
%
\begin{equation*}
  \fdmap{\transmat}{\tbvec\mu}{\tbvec{\aidx\mu}}\,.
\end{equation*}

\theme{Unique transformation?} No! \emph{Any} linearly independent linear combination of old basis vectors may be selected as the new basis vectors. The transformation is only required to be nonsingular.

\theme{Inversion:} since the transformation matrix is assumed to be nonsingular, the transformation may be inverted:
%
\begin{equation}\label{eq:inversechangeofbasisvectors}
  \tbvec\mu = \ttransmat{\aidx\mu}{\mu}\tbvec{\aidx\mu}\,,\quad\text{where}\quad
  \ttransmat{\mu}{\aidx\mu}\ttransmat{\aidx\mu}{\mu} \defas\tkron\mu\mu\,.
\end{equation}

\theme{Index place:} in our notation, \cref{eq:changeofbasisvectors,eq:inversechangeofbasisvectors} , the inverse matrix places the prime on the other index.

\theme{Careful!} Primed indices are never summed together with unprimed indices.

\theme{Dual basis?} If we change basis vectors, then we \emph{must} also transform the basis covectors, to preserve the duality condition, \cref{eq:relationbasisvecbasiscovec}:
%
\begin{equation}\label{eq:transformationofdualbasis}
  \tbcov{\aidx\mu} = \ttransmat{\aidx\mu}{\mu}\tbcov\mu\,.
\end{equation}
%
Note that the same process as in the case of transforming basis vectors applies. We are given $\tbcov\mu$. Then, transform it to have $\tbcov\mu\ttransmat{\aidx\mu}{\mu}$, which finally yields the transformed basis $\tbcov{\aidx\mu}$.

\theme{Alternative way!} Find the transformation matrix and its inverse by scalar products of the old and new basis vectors and basis covectors:
%
\begin{equation}\label{eq:transformationmatrixfromscalarproducts}
  \ttransmat\mu{\aidx\mu} = \sprod{\tbcov\mu}{\tbvec{\aidx\mu}}\quad\text{and}\quad
  \ttransmat{\aidx\mu}\mu = \sprod{\tbcov{\aidx\mu}}{\tbvec\mu}\,.
\end{equation}

\theme{Crucial!} Apart from the basis vectors and basis covectors, a vector $\vec v$ and a covector $\cov p$ are, \emph{by definition}, invariant under a change of basis. However, their components are \emph{not}! 

\theme{Vector component transformation:} Consider a vector $\vec v$. Using \cref{eq:inversechangeofbasisvectors} or \cref{eq:transformationmatrixfromscalarproducts}, we get
%
\begin{equation}\label{eq:transformationofvectorcomponents}
  \vec a = \tvec a\mu\tbvec\mu = \tvec a{\aidx\mu}\tbvec{\aidx\mu}\quad\text{and}\quad
  \tvec a{\aidx\mu} = \sprod{\tbvec{\aidx\mu}}{\vec a} = \ttransmat{\aidx\mu}{\mu}\tvec a\mu\,.
\end{equation}

\theme{Proofs:} for \cref{eq:transformationofvectorcomponents}, transform the components of $\vec a$ by applying to it the transformation matrix:
%
\begin{equation*}
  \fdmap{\transmat}{\tvec a\mu\tbvec\mu}{\tvec a\mu\ttransmat{\aidx\mu}{\mu}\tbvec{\aidx\mu} = \tvec a{\aidx\mu}\tbvec{\aidx\mu}}\,.
\end{equation*}
%
Transform the components of $\vec a$ by using the scalar product:
%
\begin{equation*}
  \tvec a{\aidx\mu} = \sprod{\tbcov{\aidx\mu}}{\tvec a\mu}
                    = \tvec a\mu\sprod{\tbcov{\aidx\mu}}{\tbvec\mu}
                    = \ttransmat{\aidx\mu}{\mu}\tvec a{\aidx\mu}\,.
\end{equation*}
%
\theme{Tip:} transforming vector components is a two step process: i) transform the basis vector from the old to the new basis and ii) transform the components of the vector using ESC. For instance, consider a vector $\vec a$ with expansion $\tvec a\mu$ on the basis $\tbvec\mu$. Transform the components of $\vec a$ to a new basis $\tbvec{\aidx\mu}$:
%
\begin{align*}
  \tbvec\mu & \mapsto\ttransmat{\aidx\mu}\mu\tbvec{\aidx\mu}\,, 
    && \eqtxt{basis vectors from old to new} \\
  %
  \tvec a\mu\ttransmat{\aidx\mu}\mu\tbvec{\aidx\mu} & \mapsto\tvec a{\aidx\mu}\tbvec{\aidx\mu}\,. 
    && \eqtxt{vector components: ESC}
\end{align*}
%
If we combine the two steps into one, then we have
%
\begin{equation*}
  \vec a = \tvec a\mu\tbvec\mu
         = \tvec a\mu\ttransmat{\aidx\mu}{\mu}\tbvec{\aidx\mu}
         = \br{\tvec a\mu\ttransmat{\aidx\mu}{\mu}}\tbvec{\aidx\mu}
         = \tvec a{\aidx\mu}\tbvec{\aidx\mu}\,.
\end{equation*}
%
\theme{Remember} that $\vec a$ does \emph{not} change, its \emph{components} do.

\theme{Note:} vector components transform oppositely to the basis vectors, \cref{eq:changeofbasisvectors}; while covectors transform like basis vectors -- as suggested by the fact that both are labeled with a subscript:
%
\begin{equation}\label{eq:transformationofcovectorcomponents}
  \cov a = \tcov a\mu\tbcov\mu = \tcov a{\aidx\mu}\tbcov{\aidx\mu}\quad\text{and}\quad
  \tcov a{\aidx\mu} = \sprod{\cov a}{\tbcov{\aidx\mu}} = \ttransmat{\mu}{\aidx\mu}\tcov a{\aidx\mu}\,.
\end{equation}
%
\theme{Tip:} transforming covector components is a two step process alike transforming vector components. These two process can be combined into one:
%
\begin{equation*}
  \cov a = \tcov a\mu\tbcov\mu
         = \tcov a\mu\ttransmat{\mu}{\aidx\mu}\tbcov{\aidx\mu}
         = \br{\tcov a\mu\ttransmat\mu{\aidx\mu}}\tbcov{\aidx\mu}
         = \tcov a{\aidx\mu}\tbcov{\aidx\mu}\,.
\end{equation*}

\theme{Using ESC.} ESC can help us to notice how basis vectors, vector components and covector components transform:
%
\begin{itemize}
  \item basis vectors change as $\tbvec{\color{red}\mu}\ttransmat{\color{red}\mu}{\aidx\mu}\mapsto\tbvec{\aidx\mu}$; they use the transformation matrix $\ttransmat{\color{red}\mu}{\aidx\mu}$;
  \item vector components change as $\tvec a{\color{red}\mu}\ttransmat{\aidx\mu}{\color{red}\mu}\mapsto\tvec a{\aidx\mu}$; they eat up the \emph{lower} index of $\ttransmat{\aidx\mu}{\color{red}\mu}$; and
  \item covector components change as $\tcov a{\color{red}\mu}\ttransmat{\color{red}\mu}{\aidx\mu}\mapsto\tcov a{\aidx\mu}$; they eat up the \emph{upper} index of $\ttransmat{\color{red}\mu}{\aidx\mu}$.
\end{itemize}
%
Thus, since basis vectors and covector components take the \emph{upper} index of $\ttransmat{\color{red}\mu}{\aidx\mu}$, they are say to transform in an \emph{alike} manner.

\theme{Note} also that the transformation matrix itself is different for vector components and covector components. For vector components, the transformation matrix is $\ttransmat{\color{red}\aidx\mu}{\mu}$; while, for covector components, $\ttransmat{\mu}{\color{red}\aidx\mu}$, the same as for basis vectors.

\theme{Naming:} Since vector components transform oppositely to the basis vectors, they are also called \lingo{contravariant} components; while covector components are called \lingo{covariant} components, for they transform alike basis vectors.

\theme{Invariance:} if the components of two vectors or two covectors are equal in one basis, then they are equal in any basis.

\theme{Tensors:} higher order tensor \emph{components} also transform under a change of basis. The new components may be found by recalling that a $\torder mn$ tensor is a function of $m$ vectors and $n$ covectors and that its components are gotten by evaluating the tensor using the basis vectors and covectors, \cref{eq:metricinversemetricidentitytensorscomponents}. For example, the metric components are transformed under the change of basis \cref{eq:changeofbasisvectors} to
%
\begin{equation}\label{eq:metriccomponentstransformation}
  \tmetric{\aidx\mu}{\aidx\nu} \defas \metric\vat{\tbvec{\aidx\mu},\tbvec{\aidx\nu}}
                                = \tmetric\alpha\beta\,\tbcov\alpha\vat{\tbvec{\aidx\mu}}\tbcov\beta\vat{\tbvec{\aidx\nu}}
                                = \tmetric\alpha\beta\,\ttransmat{\alpha}{\aidx\mu}\ttransmat{\beta}{\aidx\nu}\,.
\end{equation}
%
\theme{Tip:} for the metric tensor components: to transform from the basis $\mu,\nu$ to the basis $\aidx\mu,\aidx\nu$, we work exactly as in the case of covectors: we use the transformation matrix and ESC:
%
\begin{equation*}
  \tmetric\mu\nu\,\ttransmat{\mu}{\aidx\mu}\ttransmat{\nu}{\aidx\nu} = \tmetric{\aidx\mu}{\aidx\nu}\,.
\end{equation*}

\theme{Recall} that \lingo{evaluating} a covector or vector means using the scalar product, \cref{eq:scalarproductcovectoronvector}.

\theme{See} that the covariant components of the metric tensor (lower indices) transform exactly as covector components; \ie, they use the transformation matrices $\ttransmat{\color{red}\mu}{\aidx\mu}\ttransmat{\color{red}\nu}{\aidx\nu}$ (old basis index up, new basis index down, take upper index).

\theme{Pattern:} the components of a $\torder mn$ tensor transform like the product of $m$ vector components and $n$ covector components.

\subsubsection{Coordinate bases}
%
\theme{Coordinate system} is a set of four differentiable \emph{scalar} field $\tvec\pos\mu$ (\emph{not} one \emph{vector} field -- note that $\mu$ labels the coordinates and not the vector components) that attach a unique set of labels to each spacetime event $\event P$.

\theme{Uniqueness:} no two points are allowed to have identical values of all four scalar fields and the coordinates must vary smoothly throughout spacetime (although occasional singularities may arise).

\theme{No other restrictions} are imposed to coordinates.

\theme{Important!} The freedom to choose different coordinate systems is available to us even in a Euclidean space; there is nothing sacred about rectangular (Cartesian) coordinates. This is more true in a non-Euclidean space, where rectangular coordinates covering the whole space do not exist.

\theme{Usefulness:} coordinate systems are useful for three reasons. First, they allow us to label each spacetime point (event) by a $4$-tuple $\tuple{\tvec\pos 0, \tvec\pos 1, \tvec\pos 2, \tvec\pos 3}$. The second, and more important use, is in providing a special set of basis vectors called a \lingo{coordinate basis}. The third is that they can be used to describe the distance between two events (spacetime points).

\theme{Coordinate basis:} suppose that two infinitesimally close events have coordinates $\tvec\pos\mu$ and $\tvec\pos\mu + d\tvec\pos\mu$. The infinitesimal difference vector between the events, denoted $d\vec\pos$, is a vector defined at $\event P$. We \emph{define} the coordinate basis as the set of four basis vectors $\tbvec\mu$ such that the components of $d\vec\pos$ are $d\tvec\pos\mu$:
%
\begin{equation}\label{eq:definitionofcoordinatebasis}
  d\vec\pos \defas d\tvec\pos\mu\tbvec\mu\qquad\text{defines $\tbvec\mu$ in a coordinate basis}.
\end{equation}
%
\theme{No restrictions used.} According to \cref{eq:definitionofcoordinatebasis}, the basis vector $\tbvec 0$, for example, must point in the direction of increasing $\tvec\pos 0$ at $\event P$. This corresponds to a unique direction in (four-dimensional) spacetime just as the direction of increasing latitude corresponds to a unique direction (north) at a given point on the earth. In more math treatments, $\tbvec\mu$ is associated with the directional derivative $\ipd\mu\defas\partial/\partial\tvec\pos\mu$ at $\event P$.

\theme{Note} that \emph{not} all bases are coordinate bases.

\theme{Dual coordinate basis:} the coordinate basis $\bset{\tbvec\mu}{0}{3}$ defined by \cref{eq:definitionofcoordinatebasis} has a dual basis of covectors $\bset{\tbcov\mu}{0}{3}$ defined by \cref{eq:relationbasisvecbasiscovec}. The covector dual basis is related to the gradient as follows.

\theme{Dual coordinate basis and gradient.} Consider any scalar field $f$. Treating $f$ as a function of the coordinates, the difference in $f$ between two infinitesimally close points is
%
\begin{equation}\label{eq:definitionofgradient}
  df = \dfrac{\partial f}{\partial\tvec\pos\mu}d\tvec\pos\mu
     \defas\ipd\mu f\,d\tvec\pos\mu\,.
\end{equation}

\theme{Motivation:} consider a scalar field $f$ (that is, $\fmap{\tvec\pos\mu}{f\vat{\tvec\pos\mu}}$). Then, find its (total) differential in traditional notation and in rectangular coordinates as:
%
\begin{align*}
  df &=   \dfrac{\partial f}{\partial t}    dt
        + \dfrac{\partial f}{\partial\xpos} d\xpos
        + \dfrac{\partial f}{\partial\ypos} d\ypos
        + \dfrac{\partial f}{\partial\zpos} d\zpos\,,        &&\eqtxt{traditional notation} \\
     &=   \dfrac{\partial f}{\partial\tvec\pos 0} d\tvec\pos 0
        + \dfrac{\partial f}{\partial\tvec\pos 1} d\tvec\pos 1
        + \dfrac{\partial f}{\partial\tvec\pos 2} d\tvec\pos 2
        + \dfrac{\partial f}{\partial\tvec\pos 3} d\tvec\pos 3 \,,                  &&\eqtxt{index notation} \\
     &= \sum_{\mu = 0}^{3}\dfrac{\partial f}{\partial\tvec\pos\mu} d\tvec\pos\mu\,, &&\eqtxt{sigma notation} \\
     %
     &= \dfrac{\partial f}{\partial\tvec\pos\mu} d\tvec\pos\mu\,,  &&\eqtxt{Einstein's summation convention} \\
     %
     &= \ipd\mu f\,d\tvec\pos\mu\,.       &&\eqtxt{Jacobi's notation for partials}
\end{align*}

\Cref{eq:definitionofgradient} may be taken as the definition of the components of the gradient. However, partial derivatives depend on the coordinates, while the gradient (covariant derivative) should not. The notation $\ipd\mu$, with its covariant (subscript) index, reinforces our view that the partial derivative is the component of a covector and not a vector. 

\theme{Gradient covector:} we denote the gradient covector by $\gradcov$.

\theme{Gradient expansion:} like all covectors, the gradient may be decomposed into a sum over basis covectors, $\tbcov\mu$. Using \cref{eq:definitionofgradient,eq:relationbasisvecbasiscovec}, as the requirement for a dual basis, we conclude that the expansion of the gradient is
%
\begin{equation}\label{eq:gradientexpansiononcoordinatebasis}
  \gradcov \defas \tbcov\mu\ipd\mu\qquad\text{in a coordinate basis}\,.
\end{equation}

\theme{Crucial:} we must write the basis covector \emph{to the left} of the partial derivative operator, for the basis covector itself may depend on position! 

\theme{Coordinate free gradient.} In the present case, it is clear from its definition, \cref{eq:definitionofgradient}, that we must let the derivative act only on the function $f$. We can now rewrite \cref{eq:gradientexpansiononcoordinatebasis} in the \emph{coordinate-free manner}:
%
\begin{equation}\label{eq:coordinatefreegradient}
  df = \sprod{\gradcov f}{d\vec\pos}\,.
\end{equation}

\theme{Directional derivative:} if we want the directional derivative of $f$ along any particular direction, we simply replace $d\vec\pos$ by a vector pointing in the desired direction; \eg, the tangent vector to some curve.

\theme{Basis covector.} if we let $f$ equal one of the coordinates, using \cref{eq:gradientexpansiononcoordinatebasis}, the gradient gives us the corresponding basis covector:
%
\begin{equation}\label{eq:coordinatebasiselements}
  \gradcov = \tbcov\mu\qquad\text{in a coordinate basis}\,.
\end{equation}

\theme{Distance between events:} as mentioned, coordinates can be used to describe the distance between two events (spacetime points). However, coordinates alone are not enough, the metric tensor is also needed.

\theme{Squared distance between events} is given by
%
\begin{equation}\label{eq:squareddistancebetweenevents}
  \sqdifdist = \norm{d\vec\pos}^2
             \defas \metric\vat{d\vec\pos,d\vec\pos}
             = d\vec\pos\iprod d\vec\pos\,.
\end{equation}

\theme{Invariance:} \cref{eq:squareddistancebetweenevents} is a scalar equation that makes no reference to components, is takes as the \emph{definition} of the metric tensor.

\theme{Metric tensor uniqueness:} up to now, the metric tensor could have been any symmetric $\torder 20$ tensor. But if we insist on being able to measure distances, given an infinitesimal difference vector $d\vec\pos$, only one $\torder 20$ tensor can give the squared distance. We define the metric tensor to be that tensor. Indeed, the squared magnitude of \emph{any} vector is
%
\begin{equation*}
  \norm{\vec a}^2 \defas \metric\vat{\vec a, \vec a}\,.
\end{equation*}

\theme{Line element:} we now specialized to a coordinate basis, using \cref{eq:definitionofcoordinatebasis} for $d\vec\pos$. In a coordinate basis (and \emph{only} in a coordinate basis), the squared distance is called the \lingo{line element} and takes the form, by \cref{eq:metricinversemetricidentitytensorscomponents}:
%
\begin{equation}\label{eq:lineelementdefinition}
  \sqdifdist = \tmetric\mu\nu\,d\tvec\pos\mu d\tvec\pos\nu\qquad\text{in a coordinate basis}\,.
\end{equation}

\theme{Change of basis.} If we transform coordinates, we will have to change our vector and covector bases. Suppose that we transform from $\tvec\pos\mu$ to $\tvec\pos{\aidx\mu}$, with the prime indicating the new coordinates. For instance, in the Euclidean plane, we could transform from rectangular coordinates ($\tvec\pos 1 = \xpos, \tvec\pos 2 = \ypos$) to polar coordinates ($\tvec\pos{\aidx 1} = \cxpos,\tvec\pos{\aidx 2} = \cypos$):
%
\begin{equation*}
  \xpos = \cxpos\cos\cypos\qquad\text{and}\qquad
  \ypos = \cxpos\sin\cypos\,.
\end{equation*}
%
A one-to-one mapping is given from the old to new coordinates, allowing us to define \lingo{Jacobi's matrix} and its inverse, \lingo{Jacobi's inverse matrix} as
%
\begin{equation*}
  \ttransmat{\aidx\mu}{\nu}\defas\ipd\nu\tvec\pos{\aidx\mu}
    \parens{= \dfrac{\partial\tvec\pos{\aidx\mu}}{\partial\tvec\pos\nu}}
  \qquad\text{and}\qquad
  \ttransmat{\nu}{\aidx\mu} = \ipd{\aidx\mu}\tvec\pos\nu\,.
\end{equation*}

\theme{Vector components transformation:} see that vector \emph{components} transform like $d\tvec\pos{\aidx\mu} = \ipd\nu\tvec\pos{\aidx\mu}\,d\tvec\pos\nu$.

\theme{Other transformation:} transforming the basis vectors, basis covectors, and tensor components is straightforward using \crefrange{eq:changeofbasisvectors}{eq:metriccomponentstransformation}.

\theme{Invariant:} \cref{eq:definitionofcoordinatebasis,eq:gradientexpansiononcoordinatebasis,eq:coordinatebasiselements,eq:lineelementdefinition} remain valid after a coordinate transformation.

\theme{Proofs.} we have that
%
\begin{align*}
  d\tvec\pos\mu\tbvec\mu &\mapsto d\tvec\pos\mu\tbvec{\aidx\mu}\ttransmat{\aidx\mu}{\mu} 
    = d\tvec\pos{\aidx\mu}\tbvec{\aidx\mu}\,, &&\qquad\eqtxt{\cref{eq:definitionofcoordinatebasis}} \\
  %
  \tbcov\mu\ipd\mu &\mapsto\tbcov{\aidx\mu}\ttransmat{\mu}{\aidx\mu}\ipd\mu 
    = \tbcov{\aidx\mu}\ipd{\aidx\mu}\,, &&\qquad\eqtxt{\cref{eq:gradientexpansiononcoordinatebasis}} \\
  %
  \tmetric\mu\nu\,d\tvec\pos\mu d\tvec\pos\nu
    &\mapsto\tmetric{\aidx\mu}{\aidx\nu}\ttransmat{\aidx\mu}{\mu}\ttransmat{\aidx\nu}{\nu}\,d\tvec\pos\mu d\tvec\pos\nu
  = \tmetric{\aidx\mu}{\aidx\nu}\,d\tvec\pos{\aidx\mu}d\tvec\pos{\aidx\nu}\,; &&\qquad\eqtxt{\cref{eq:lineelementdefinition}}
\end{align*}
%
\ie, they are all valid after transformations.

\theme{Reflection.} Let us reflect on our treatment of vectors, covectors, and tensors. The math and notation, while straightforward, are complicated. Can we simplify the notation without sacrificing rigor? One way to modify our notation would be to abandon the basis vectors and covectors and to work only with components of tensors. We could have defined vectors, covectors, and tensors from the outset in terms of the transformation properties of their components. However, the reader should appreciate the clarity of the geometrical approach that we have adopted. \emph{Our notation has forced us to distinguish physical objects like vectors from basis-dependent ones, like vector components}. As long as the definition of a tensor is not forgotten, computations are straightforward and unambiguous. Moreover, adopting a basis did not force us to abandon geometrical concepts. On the contrary, computations are made easier and clearer by retaining the notation and meaning of basis vectors and covectors.


\subsubsection{Isomorphism of vectors and covectors}
%
\theme{Different but related.} The linear space of vectors is \emph{isomorphic} to the dual vector space of covectors; thus, every equation or operation in one space has an equivalent equation or operation in the other space.


\subsubsection{Example: Euclidean plane}
%
\theme{Description.} The Euclidean plane is a flat two-dimensional space that can be covered by rectangular coordinates $\tuple{\xpos,\ypos}$ with line element and metric components:
%
\begin{equation}\label{eq:euclideanplanelineelementmetricrectangular}
  \sqdifdist = \br{d\xpos}^2 + \br{d\ypos}^2 \implies
  \tmetric\xpos\xpos = \tmetric\ypos\ypos = 1\,,\quad\text{and}\quad
  \tmetric\xpos\ypos = \tmetric\ypos\xpos = 0\,.
\end{equation}

\theme{Polar coordinates.} However, there is nothing sacred about rectangular coordinates. Consider polar coordinates $\tuple{\cxpos,\cypos}$, defined by the transformation $x = \cxpos\cos\cypos, y= \cxpos\sin\cypos$. An exercise in partial derivatives yields the line element and from it the metric tensor coefficients in polar coordinates:
%
\begin{equation}\label{eq:euclideanpolarcoordinateslineelementmetric}
  \sqdifdist = \br{d\cxpos}^2 + \br{\cxpos d\cypos}^2 \implies
  \tmetric\cxpos\cxpos = 1 \,,\quad
  \tmetric\cypos\cypos = \br{\cxpos}^2\,,\quad\text{and}\quad
  \tmetric\cxpos\cypos = \tmetric\cypos\cxpos = 0\,.
\end{equation}

\theme{Coordinate basis are not normal.} This appears reasonable until one considers the basis vectors $\tbvec\cxpos,\tbvec\cypos$, recalling that $\tmetric\mu\nu = \tbvec\mu\iprod\tbvec\nu$. Then, while $\tbvec\cxpos\iprod\tbvec\cxpos = 1$ and $\tbvec\cxpos\iprod\tbvec\cypos = 0$; $\tbvec\cypos\iprod\tbvec\cypos = \cxpos^2 \neq 1$, thus $\tbvec\cypos$ is \emph{not} a unit vector. The new basis vectors are found in terms of the rectangular basis using \cref{eq:changeofbasisvectors}:
%
\begin{equation}\label{eq:polarbasisinrectangularbasis}
  \tbvec\cxpos = \dfrac{\xpos}{\sqrt{\xpos^2 + \ypos^2}}\tbvec\xpos 
               + \dfrac{\ypos}{\sqrt{\xpos^2 + \ypos^2}}\tbvec\ypos\,,\quad
  \tbvec\cypos = - \ypos\tbvec\xpos + \xpos\tbvec\ypos\,.
\end{equation}
%
The polar \emph{unit} vectors are $\uvec\cxpos = \tbvec\cxpos, \uvec\cypos = \inv\cxpos\tbvec\cypos$.

\theme{Reflect.} Why does our formalism give us non-unit vectors? The answer is because we \emph{insisted} that our basis vectors be a \emph{coordinate} basis (\cref{eq:definitionofcoordinatebasis,eq:gradientexpansiononcoordinatebasis,eq:coordinatebasiselements,eq:lineelementdefinition}). 

\theme{Simpler form in coordinate basis:} in terms of the orthonormal unit vectors, the difference position (vector) between points $\tuple{\cxpos,\cypos}$ and $\tuple{\cxpos + d\cxpos, \cypos + d\cypos}$ is $d\vec\pos = \uvec\cxpos\,d\cxpos + \uvec\cypos\,\cxpos d\cypos$. In the \emph{coordinate basis}, this takes the simpler form $d\vec\pos = \tbvec\cxpos\,d\cxpos + \tbvec\cypos\,d\cypos = d\tvec\pos\mu\tbvec\mu$.

\theme{Metric tensor measures lengths.} In the coordinate basis, we don't have to worry about normalizing our vectors: \emph{all information about lengths is carried instead by the metric tensor.} In the \emph{non}-coordinate basis of orthonormal vectors $\uvec\cxpos,\uvec\cypos$, we have to make a separate note that the distance elements are $d\cxpos,\cxpos d\cypos$.

In the non-coordinate basis, we can no longer use \cref{eq:lineelementdefinition} for the line element. We must instead use \cref{eq:squareddistancebetweenevents}. The metric components in the non-coordinate basis are
%
\begin{equation}\label{eq:metriccomponentsinnoncoordinatebasis}
  \tmetric{\uvec\cxpos}{\uvec\cxpos} = \tmetric{\uvec\cypos}{\uvec\cypos} = 1\quad\text{and}\quad
  \tmetric{\uvec\cxpos}{\uvec\cypos} = \tmetric{\uvec\cypos}{\uvec\cxpos} = 0\,.
\end{equation}

\theme{Useless metric.} Thus, in a non-coordinate basis, the metric tensor will not tell us how to measure distances in terms of coordinate differentials.

With a non-coordinate basis, we must sacrifice \cref{eq:definitionofcoordinatebasis,eq:lineelementdefinition}. Nonetheless, for some applications, it proves convenient to introduce an orthonormal non-coordinate basis called a \lingo{tetrad basis}.

\theme{Non-coordinates and gradient.} The use of non-coordinate bases also complicates the gradient (\cref{eq:gradientexpansiononcoordinatebasis,eq:coordinatebasiselements}). In our polar \emph{coordinate} basis (\cref{eq:euclideanpolarcoordinateslineelementmetric}), the inverse metric components are
%
\begin{equation}\label{eq:invmetriccomponentsincoordinatebasis}
  \tinvmet\cxpos\cxpos = 1\,,\quad
  \tinvmet\cypos\cypos = \br{\xpos}^{-2}\,,\quad\text{and}\quad
  \tinvmet\cxpos\cypos = \tinvmet\cypos\cxpos = 0\,.
\end{equation}
%
\theme{Tip:} The matrix $\tmetric\mu\nu$ is diagonal, so its inverse is also diagonal with entries given by the reciprocals.

The basis covectors obey the rules $\tbcov\mu\iprod\tbcov\nu = \tinvmet\mu\nu$. They are isomorphic to the dual basis vectors $\tbvec\mu = \tinvmet\mu\nu\tbvec\nu$. Thus, $\tensor{\bvec}{^\cxpos} = \tbvec\cxpos = \uvec\cxpos$, $\tensor{\bvec}{^\cypos} = \cxpos^{-2}\tbvec\cypos = \inv\cxpos\uvec\cypos$. \Cref{eq:gradientexpansiononcoordinatebasis} gives the gradient covector as $\gradcov = \tbcov\cxpos\ipd\cxpos + \tbcov\cypos\ipd\cypos$. Expressing this as a vector, we get
%
\begin{equation}\label{eq:gradientvectorinpolarcoordinates}
  \gradvec = \tbvec\cxpos\ipd\cxpos + \tbvec\cypos\ipd\cypos
           = \uvec\cxpos\ipd\cxpos + \uvec\cypos\dfrac 1\cxpos\ipd\cypos\,.
\end{equation}
%
\theme{Simpler gradient.} The gradient is simpler is the coordinate basis. The coordinate basis has the added advantage that we can get the dual basis vectors (or the basis covectors) by applying the gradient to the coordinates:
%
\begin{equation*}
  \tbvec\cxpos = \gradvec\cxpos\qquad\text{and}\qquad
  \tbvec\cypos = \gradvec\cypos\,.
\end{equation*}

\theme{Convention:} from now on, unless otherwise noted, we will assume that our basis vectors are on a coordinate basis. We will use covectors and vectors interchangeably through the mapping provided by the metric tensor and its inverse.


\subsection{Differentiation and integration}
%

